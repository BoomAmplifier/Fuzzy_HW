{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cac4537-15e0-4ef1-a552-198ddbc8b734",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **HW5 for Fuzzy System** \n",
    "\n",
    "> Instructor: Dr. Hao Ying     \n",
    "> **Solutions given by Yuqi Wang**   \n",
    "> Email: he1173@wayne.edu     \n",
    "> Date: 03.01.2022   \n",
    "\n",
    "## Descriptions\n",
    "- Source codes will be uploaded and updated to [Github repo](https://github.com/BoomAmplifier/Fuzzy_HW).     \n",
    "- *Matlab* is licenced by WSU.    \n",
    "- *Jupyter Notebook* and *python3* are also used in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660175b8-3860-4679-b232-2da059951f25",
   "metadata": {},
   "source": [
    "## Solutions  \n",
    "   \n",
    "#### Problem 1 (Handout-E2.1):   \n",
    "A single input neuron has a weight of 1.3 and a bias of 3.0. What possible kinds of transfer functions, from Table 2.1, could this neuron have, if its output is given below. In each case, give the value of the input that would produce these outputs.\n",
    "- 1) 1.6 \n",
    "- 2) 1.0 \n",
    "- 3) 0.9963 \n",
    "- 4) -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b180c-fc66-4976-b16a-259fc7306352",
   "metadata": {},
   "source": [
    "**Solution:**    \n",
    "The net input to the transfer function is given by: \n",
    "$$n = wp + b = (1.3)p + 3.0$$ \n",
    "- 1) $a=1.6$   \n",
    "According to Table 2.1, the transfer function could be:\n",
    "   - Linear\n",
    "   $$a=\\rm{purelin}(n)=1.6$$\n",
    "   $$n=1.6 \\Rightarrow p=-14/13$$   \n",
    "   - Positive Linear\n",
    "   The same as Linear, $p=2$.\n",
    "As for Log-Sigmoid and Hyperbolic Tangent Sigmoid, due to their reults should be less than 1, they donot match $a=1.6$.   \n",
    "- 2) $a=1.0$ \n",
    "   - Hard Limit or Symmetrical Hard Limit \n",
    "   $$n\\geq 0 \\Rightarrow p\\geq -\\frac{3}{1.3}$$  \n",
    "   - Linear\n",
    "   $$n=1 \\Rightarrow p=-2/1.3$$  \n",
    "   - Saturating Linear or Symmetric Saturating Linear \n",
    "   $$n>1 \\Rightarrow p>-2/1.3$$ \n",
    "   - Positive Linear\n",
    "   $$n=1 \\Rightarrow p=-2/1.3$$ \n",
    "- 3) $a=0.9963$\n",
    "   - Linear or Positive Linear\n",
    "   $$n=0.9963 \\Rightarrow p=-2.0037/1.3$$  \n",
    "   - Log-Sigmoid\n",
    "   $$\\frac{1}{1+e^{-n}}=0.9963 \\Rightarrow n\\approx-5.6 \\Rightarrow p \\approx = -6.61$$ \n",
    "   - Hyperbolic Tangent Sigmoid\n",
    "   $$\\frac{e^n-e^{-n}}{e^n+e^{-n}}=0.9963 \\Rightarrow n\\approx 6.29 \\Rightarrow p \\approx = 2.53$$ \n",
    "- 4) $a=-1$\n",
    "   - Symmetrical Hard Limit\n",
    "   $$n< 0 \\Rightarrow p< -\\frac{3}{1.3}$$ \n",
    "   - Symmetrical Saturating Linear\n",
    "   $$n< -1 \\Rightarrow p< -\\frac{4}{1.3}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb96f46f-4c7c-442e-97d0-9df9e9fe26e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Problem 2 (Handout-E2.2):  \n",
    "Consider a single-input neuron with a bias. We would like the output to be -1 for inputs less than 3 and +1 for inputs greater than or equal to 3. \n",
    "- 1) What kind of a transfer function is required?   \n",
    "- 2) What bias would you suggest? Is your bias in any way related to the input weight? If yes, how?   \n",
    "- 3) Summarize your network by naming the transfer function and stating the bias and the weight. Draw a diagram of the network. Verify the network performance using MATLAB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0895e78f-4cac-4008-b16f-4ad366655ff5",
   "metadata": {},
   "source": [
    "**Solution:**    \n",
    "- 1) The transfer function could be hardlims.\n",
    "- 2) $a=-1, \\text{when } p<3; a=1, \\text{when } p\\geq 3$    \n",
    "$$p < 3,n < 0 \\Rightarrow wp+b<0 \\Rightarrow b<-3w$$\n",
    "$$p \\geq 3,n \\geq 0 \\Rightarrow wp+b\\geq0 \\Rightarrow b\\geq-3w$$ \n",
    "- 3) Let $w=1,b=-3$, the neuron is as follows.\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "    <img src=\".\\fig\\hw5-1.png\" style=\"width: 40%;height: 40%\"/>\n",
    "</center>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79254da-de82-4e0a-a0ab-aeb710c9e5d3",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<center>\n",
    "    <img src=\".\\fig\\hw5-4.jpg\" style=\"width: 40%;height: 40%\"/>\n",
    "</center>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928f43b-09a6-4be4-87ac-d9057fbb34ed",
   "metadata": {},
   "source": [
    "#### Problem 3 (Handout-E2.4):  \n",
    "A two-layer neural network is to have four inputs and six outputs. The range of the outputs is to be continuous between 0 and 1. What can you tell about the network architecture? Specifically:\n",
    "- 1) How many neurons are required in each layer?  \n",
    "- 2) What are the dimensions of the first-layer and second-lay weight matrices?  \n",
    "- 3) What kinds of transfer functions can be used in each layer?  \n",
    "- 4) Are biases required in either layer?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4e3efd-3007-4420-9d38-59e1a9cd4780",
   "metadata": {},
   "source": [
    "**Solution:**   \n",
    "- 1) For the second layer, it requires 6 neurons, one for each output. But for the first layer, it is hard to determine.\n",
    "- 2) Let $S^1$ be the number of neurons in the first layer, then the dimension of the first-layer weight matrices is $S^1 \\times 4$, while the dimension of the second-layer weight matrices is $6\\times S^1 $. \n",
    "- 3) Saturation Linear can be used in the second layer. The transfer function for the first layer could be Linear, Saturation Linear, Symmetric Saturating Linear, Log-Sigmoid, Hyperbolic Tangent Sigmoid, and Positive Linear.\n",
    "- 4) Not enough information is given to determine if a bias is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f959601-1c55-41ae-90f0-6823ec741f53",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Problem 4 (Handout-E2.5):  \n",
    "Consider the following neuron.   \n",
    "\n",
    "<div>\n",
    "<center>\n",
    "    <img src=\".\\fig\\hw5-1.jpg\" style=\"width: 40%;height: 40%\"/>\n",
    "</center>\n",
    "</div> \n",
    "\n",
    "Sketch the neuron response (plot $a$ versus $p$ for $-2<p<2$) for the following cases.   \n",
    "- 1) $w=1, b=1, f=hardlims$   \n",
    "- 2) $w=-1, b=1, f=hardlims$   \n",
    "- 3) $w=2, b=3, f=purelin$   \n",
    "- 4) $w=2, b=3, f=satlins$   \n",
    "- 5) $w=-2, b=-1, f=poslin$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0219f9c9-d677-4017-aa08-a34333470652",
   "metadata": {},
   "source": [
    "**Solution:**    \n",
    "- 1) $a=f(wp+b)=\\rm{hardlims}(p+1)$\n",
    "\n",
    "- 2) $a=f(wp+b)=\\rm{hardlims}(-p+1)$\n",
    "\n",
    "- 3) $a=f(wp+b)=\\rm{purelin}(2p+3)$ \n",
    "\n",
    "- 4) $a=f(wp+b)=\\rm{satlins}(2p+3)$\n",
    "\n",
    "- 5) $a=f(wp+b)=\\rm{poslin}(-2p-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad1c8e6-3770-4dc4-a190-c69f70b53c0f",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<center>\n",
    "    <img src=\".\\fig\\hw5-3.svg\" style=\"width: 100%;height:100%\"/>\n",
    "</center>\n",
    "</div> \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
